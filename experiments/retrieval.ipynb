{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 ____       __       _                 __   ______            _ __      __     __\n",
    "                / __ \\___  / /______(_)__ _   ______ _/ /  / ____/___  ____  (_) /___  / /_   / /\n",
    "               / /_/ / _ \\/ __/ ___/ / _ \\ | / / __ `/ /  / /   / __ \\/ __ \\/ / / __ \\/ __/  / / \n",
    "              / _, _/  __/ /_/ /  / /  __/ |/ / /_/ / /  / /___/ /_/ / /_/ / / / /_/ / /_   /_/  \n",
    "             /_/ |_|\\___/\\__/_/  /_/\\___/|___/\\__,_/_/   \\____/\\____/ .___/_/_/\\____/\\__/  (_)   \n",
    "                                                                   /_/                           \n",
    "                                       (learning every nanosecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting code\n",
    "The goal of this algorithm is to retrieve a relevant code snippet given an English description and a series of already defined code/description pairs.\n",
    "![ret](../images/retrievalHighLevel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import pyndri\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import re, string, timeit\n",
    "from colored import fg, bg, attr\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indri \n",
    "Indri is the retrieval engine that I am currently using since it has a nice interface with python and has some of the algorithms I need.\n",
    "\n",
    "Indri takes files in an XML format. Sentence pairs are usually stored line by line in a file. So we will need to convert from single line to formatted XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets \n",
    "We will currently make a full explanation for only one dataset: Django. This is because it is relatively small (18k sentences) and clean. Further descriptions and analysis are found in other notebooks in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(fp, src_ext=\".src\", tgt_ext=\".tgt\", lines=[3,21,80,99]):\n",
    "    linecache.clearcache()\n",
    "    for l in lines:\n",
    "        print(\"LINE: {} \\nSOURCE:    {} \\nTARGET:     {}\\n\".format(l, \n",
    "                                                                   linecache.getline(fp+src_ext, l), \n",
    "                                                                   linecache.getline(fp+tgt_ext, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 13 \n",
      "SOURCE:      define the function get_cache with backend and dictionary pair of elements kwargs as arguments.\n",
      " \n",
      "TARGET:         def get_cache ( backend , ** kwargs ) :\n",
      "\n",
      "\n",
      "LINE: 14 \n",
      "SOURCE:      call the function warnings.warn with string \"'get_cache' is deprecated in favor of 'caches'.\", RemovedInDjango19Warning,\n",
      " \n",
      "TARGET:      warnings . warn ( \"'get_cache' is deprecated in favor of 'caches'.\" ,  RemovedInDjango19Warning , stacklevel = 2 )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "django_fp = \"../datasets/django/all\"\n",
    "show_sample(django_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[13,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  from threading import local into default name space.\r\n",
      "  import module warnings.\r\n",
      "  from django.conf import settings into default name space.\r\n",
      "  from django.core import signals into default name space.\r\n",
      "  from django.core.cache.backends.base import InvalidCacheBackendError, CacheKeyWarning and BaseCache into default name space.\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from threading import local\r\n",
      " import warnings\r\n",
      "  from django . conf import settings\r\n",
      " from django . core import signals\r\n",
      " from django . core . cache . backends . base import (  InvalidCacheBackendError , CacheKeyWarning , BaseCache )\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  temp  Created \n"
     ]
    }
   ],
   "source": [
    "dirName = \"temp\"\n",
    " \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and train / test split\n",
    "Copy the full dataset to the temp folder. We then split the data into a training and testing set at around 90% / 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9 # this means 90% of the data will be used for training, thus 10% for testing\n",
    "num_samples = sum(1 for line in open(django_fp + \".desc\"))\n",
    "train_cutoff = int(num_samples * train_ratio)\n",
    "\n",
    "lines = np.arange(num_samples)\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "train_lines = lines[:train_cutoff]\n",
    "test_lines = lines[train_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "train_fp = \"temp/retrieval_train\"\n",
    "test_fp = \"temp/retrieval_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".code\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".desc\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".code\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TrecText format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"r\") as f, open(\"temp/train_desc.trectext\", \"w\") as out:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        \n",
    "        if not line :\n",
    "            break\n",
    "            \n",
    "        out.write(\"<DOC>\\n  <DOCNO>{}</DOCNO>\\n  <TEXT>\\n{}  </TEXT>\\n</DOC>\\n\".format(count, line))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index with indri\n",
    "To create an index we need to supply Indri with a parameter file specifying how to handle each document. Indri will then generate an index folder with is fast to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/IndriBuildIndex.conf\", \"w\") as out:\n",
    "    conf = \"\"\"\n",
    "<parameters>\n",
    "<index>temp/django_index/</index>\n",
    "<memory>1024M</memory>\n",
    "<storeDocs>true</storeDocs>\n",
    "<corpus><path>temp/train_desc.trectext</path><class>trectext</class></corpus>\n",
    "<stemmer><name>krovetz</name></stemmer>\n",
    "</parameters>\"\"\"\n",
    "    \n",
    "    out.write(conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kstem_add_table_entry: Duplicate word emeritus will be ignored.\n",
      "0:00: Created repository temp/django_index/\n",
      "0:00: Opened temp/train_desc.trectext\n",
      "0:08: Documents parsed: 16923 Documents indexed: 16923\n",
      "0:08: Closed temp/train_desc.trectext\n",
      "0:08: Closing index\n",
      "0:08: Finished\n"
     ]
    }
   ],
   "source": [
    "!IndriBuildIndex temp/IndriBuildIndex.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pyndri.Index(\"temp/django_index/\")\n",
    "env = pyndri.TFIDFQueryEnvironment(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = env.query('error handler', results_requested=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 16012 \n",
      "SOURCE:      for every handler in handlers,\n",
      " \n",
      "TARGET:          for handler in handlers :\n",
      "\n",
      "\n",
      "LINE: 10339 \n",
      "SOURCE:      for every handler in handlers,\n",
      " \n",
      "TARGET:                               for handler in handlers :\n",
      "\n",
      "\n",
      "LINE: 8784 \n",
      "SOURCE:      substitute self._upload_handlers for handlers.\n",
      " \n",
      "TARGET:      handlers = self . _upload_handlers\n",
      "\n",
      "\n",
      "LINE: 7760 \n",
      "SOURCE:      substitute upload_handlers for self._upload_handlers.\n",
      " \n",
      "TARGET:       self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n",
      "LINE: 4361 \n",
      "SOURCE:      substitute _upload_handlers for self.__upload_handlers.\n",
      " \n",
      "TARGET:      self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_sample(train_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[doc[0] for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_code = [linecache.getline(django_fp + \".code\", doc[0]) for doc in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove punctuation from input query strings so that Indri accepts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(r'[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 1 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      if p_pattern starts with a string '^',\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if token_string starts with VARIABLE_TAG_START,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m                   if token_string . startswith ( VARIABLE_TAG_START ) :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     if p_pattern . startswith ( '^' ) :\n",
      "\n",
      "\n",
      "LINE: 501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      return value.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      return value.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               return value\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      return value\n",
      "\n",
      "\n",
      "LINE: 1001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      define the method _isdst with 2 arguments self and dt.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      tt is a tuple with 9 elements: dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, result of the method dt.weekday,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               tt = ( dt . year , dt . month , dt . day ,  dt . hour , dt . minute , dt . second ,  dt . weekday ( ) , 0 , 0 )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m        def _isdst ( self , dt ) :\n",
      "\n",
      "\n",
      "LINE: 1501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      call the method handler.addQuickElement with 3 arguments: string 'link', an empty string and a dictionary with 2 entries:\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      call the method handler.addQuickElement with 3 arguments: string 'link', an empty string and a dictionary with 2 entries:\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      handler . addQuickElement ( \"link\" , \"\" , { \"rel\" : \"alternate\" , \"href\" : self . feed [ 'link' ] } )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     handler . addQuickElement ( \"link\" , \"\" , { \"href\" : item [ 'link' ] , \"rel\" : \"alternate\" } )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linecache.clearcache()\n",
    "with open(test_fp + \".desc\", \"r\") as f, open(\"temp/retrieval_predictions\" + \".code\", \"w\") as out:\n",
    "    lines = f.readlines()\n",
    "    l = 1\n",
    "    for line in lines:\n",
    "        result = env.query(p.sub(\" \", line), results_requested=1)\n",
    "        if result != ():\n",
    "            out.write(linecache.getline(train_fp + \".code\",result[0][0]))\n",
    "        else:\n",
    "            out.write(\"\\n\")\n",
    "            print(\"LINE: {} \\n{}{}QUERY:{}    {}\\nSANITIZED QUERY      {}\\n{}{}PRED DESCRIPTION{}:   ######### NO PREDICTION ##########\\n\".format(\n",
    "                l, \n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                line,\n",
    "                fg(24), \n",
    "                bg(217),\n",
    "                attr(0),\n",
    "                line.translate(str.maketrans('', '', string.punctuation))))\n",
    "        if l % 500 == 1:\n",
    "            print(\"LINE: {} \\n{}{}QUERY:{}    {}\\n{}{}PRED DESCRIPTION:{}    {} \\n{}{}PRED CODE:{}     {}\\n{}{}TRUTH:{}    {}\\n\".format(\n",
    "                l, \n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                line, \n",
    "                fg(24), \n",
    "                bg(153),\n",
    "                attr(0),\n",
    "                linecache.getline(train_fp + \".desc\", result[0][0]), \n",
    "                fg(24), \n",
    "                bg(153),\n",
    "                attr(0),\n",
    "                linecache.getline(train_fp + \".code\", result[0][0]),\n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                linecache.getline(test_fp + \".code\", l)))\n",
    "        l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_uncased =  40.40\n"
     ]
    }
   ],
   "source": [
    "from tensor2tensor.utils import bleu_hook\n",
    "\n",
    "bleu = 100 * bleu_hook.bleu_wrapper(\"temp/retrieval_test.code\", \"temp/retrieval_predictions.code\",\n",
    "                                          case_sensitive=False)\n",
    "print(\"BLEU_uncased = %6.2f\" % bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
