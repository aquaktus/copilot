{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 ____       __       _                 __   ______            _ __      __     __\n",
    "                / __ \\___  / /______(_)__ _   ______ _/ /  / ____/___  ____  (_) /___  / /_   / /\n",
    "               / /_/ / _ \\/ __/ ___/ / _ \\ | / / __ `/ /  / /   / __ \\/ __ \\/ / / __ \\/ __/  / / \n",
    "              / _, _/  __/ /_/ /  / /  __/ |/ / /_/ / /  / /___/ /_/ / /_/ / / / /_/ / /_   /_/  \n",
    "             /_/ |_|\\___/\\__/_/  /_/\\___/|___/\\__,_/_/   \\____/\\____/ .___/_/_/\\____/\\__/  (_)   \n",
    "                                                                   /_/                           \n",
    "                                       (learning every nanosecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting code\n",
    "The goal of this algorithm is to retrieve a relevant code snippet given an English description and a series of already defined code/description pairs.\n",
    "![ret](../images/retrievalHighLevel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import pyndri\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import re, string, timeit\n",
    "from colored import fg, bg, attr\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from tensor2tensor.utils import bleu_hook\n",
    "from multiprocessing import Process, Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indri \n",
    "Indri is the retrieval engine that I am currently using since it has a nice interface with python and has some of the algorithms I need.\n",
    "\n",
    "Indri takes files in an XML format. Sentence pairs are usually stored line by line in a file. So we will need to convert from single line to formatted XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets \n",
    "We will currently make a full explanation for only one dataset: Django. This is because it is relatively small (18k sentences) and clean. Further descriptions and analysis are found in other notebooks in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(fp, src_ext=\".src\", tgt_ext=\".tgt\", lines=[3,21,80,99]):\n",
    "    linecache.clearcache()\n",
    "    for l in lines:\n",
    "        print(\"LINE: {} \\nSOURCE:    {} \\nTARGET:     {}\\n\".format(l, \n",
    "                                                                   linecache.getline(fp+src_ext, l), \n",
    "                                                                   linecache.getline(fp+tgt_ext, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 13 \n",
      "SOURCE:      define the function get_cache with backend and dictionary pair of elements kwargs as arguments.\n",
      " \n",
      "TARGET:         def get_cache ( backend , ** kwargs ) :\n",
      "\n",
      "\n",
      "LINE: 14 \n",
      "SOURCE:      call the function warnings.warn with string \"'get_cache' is deprecated in favor of 'caches'.\", RemovedInDjango19Warning,\n",
      " \n",
      "TARGET:      warnings . warn ( \"'get_cache' is deprecated in favor of 'caches'.\" ,  RemovedInDjango19Warning , stacklevel = 2 )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "django_fp = \"../datasets/django/all\"\n",
    "show_sample(django_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[13,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  from threading import local into default name space.\r\n",
      "  import module warnings.\r\n",
      "  from django.conf import settings into default name space.\r\n",
      "  from django.core import signals into default name space.\r\n",
      "  from django.core.cache.backends.base import InvalidCacheBackendError, CacheKeyWarning and BaseCache into default name space.\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from threading import local\r\n",
      " import warnings\r\n",
      "  from django . conf import settings\r\n",
      " from django . core import signals\r\n",
      " from django . core . cache . backends . base import (  InvalidCacheBackendError , CacheKeyWarning , BaseCache )\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  temp  Created \n"
     ]
    }
   ],
   "source": [
    "dirName = \"temp\"\n",
    " \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and train / test split\n",
    "Copy the full dataset to the temp folder. We then split the data into a training and testing set at around 90% / 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9 # this means 90% of the data will be used for training, thus 10% for testing\n",
    "num_samples = sum(1 for line in open(django_fp + \".desc\"))\n",
    "train_cutoff = int(num_samples * train_ratio)\n",
    "\n",
    "lines = np.arange(num_samples)\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "train_lines = lines[:train_cutoff]\n",
    "test_lines = lines[train_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "train_fp = \"temp/retrieval_train\"\n",
    "test_fp = \"temp/retrieval_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".code\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".desc\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".code\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TrecText format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"r\") as f, open(\"temp/train_desc.trectext\", \"w\") as out:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        \n",
    "        if not line :\n",
    "            break\n",
    "            \n",
    "        out.write(\"<DOC>\\n  <DOCNO>{}</DOCNO>\\n  <TEXT>\\n{}  </TEXT>\\n</DOC>\\n\".format(count, line))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index with indri\n",
    "To create an index we need to supply Indri with a parameter file specifying how to handle each document. Indri will then generate an index folder with is fast to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/IndriBuildIndex.conf\", \"w\") as out:\n",
    "    conf = \"\"\"\n",
    "<parameters>\n",
    "<index>temp/django_index/</index>\n",
    "<memory>1024M</memory>\n",
    "<storeDocs>true</storeDocs>\n",
    "<corpus><path>temp/train_desc.trectext</path><class>trectext</class></corpus>\n",
    "<stemmer><name>krovetz</name></stemmer>\n",
    "</parameters>\"\"\"\n",
    "    \n",
    "    out.write(conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kstem_add_table_entry: Duplicate word emeritus will be ignored.\n",
      "0:00: Created repository temp/django_index/\n",
      "0:00: Opened temp/train_desc.trectext\n",
      "0:06: Documents parsed: 9401 Documents indexed: 9401\n",
      "0:06: Closed temp/train_desc.trectext\n",
      "0:06: Closing index\n",
      "0:06: Finished\n"
     ]
    }
   ],
   "source": [
    "!IndriBuildIndex temp/IndriBuildIndex.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pyndri.Index(\"temp/django_index/\")\n",
    "env = pyndri.TFIDFQueryEnvironment(index, k1=1.2, b=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = env.query('error handler', results_requested=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 1718 \n",
      "SOURCE:      define the method resolve_error_handler with arguments self and view_type.\n",
      " \n",
      "TARGET:        def resolve_error_handler ( self , view_type ) :\n",
      "\n",
      "\n",
      "LINE: 6919 \n",
      "SOURCE:      substitute self._upload_handlers for handlers.\n",
      " \n",
      "TARGET:      handlers = self . _upload_handlers\n",
      "\n",
      "\n",
      "LINE: 2756 \n",
      "SOURCE:      substitute upload_handlers for self._upload_handlers.\n",
      " \n",
      "TARGET:       self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n",
      "LINE: 2253 \n",
      "SOURCE:      for every handler in self._upload_handlers,\n",
      " \n",
      "TARGET:                  for handler in self . _upload_handlers :\n",
      "\n",
      "\n",
      "LINE: 1925 \n",
      "SOURCE:      substitute _upload_handlers for self.__upload_handlers.\n",
      " \n",
      "TARGET:      self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_sample(train_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[doc[0] for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_code = [linecache.getline(django_fp + \".code\", doc[0]) for doc in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove punctuation from input query strings so that Indri accepts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 1 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      if not,\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if not,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m        else :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      else :\n",
      "\n",
      "\n",
      "LINE: 501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      define the method __init__ with, self, server, params, library, value_not_found_exception as arguments.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      call method __init__ from the base class of the class PyLibMCCache with arguments: server, params,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      super ( PyLibMCCache , self ) . __init__ ( server , params ,  library = pylibmc ,  value_not_found_exception = pylibmc . NotFound )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m          def __init__ ( self , server , params , library , value_not_found_exception ) :\n",
      "\n",
      "\n",
      "LINE: 1001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      call the method self._css.keys, sort the result and substitute it for media.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      sort elements of kwds, substitute the result for sorted_items.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m                   sorted_items = sorted ( kwds . items ( ) )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m                media = sorted ( self . _css . keys ( ) )\n",
      "\n",
      "\n",
      "LINE: 1501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      model_dependencies is an empty list.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      _empty_value is an empty list.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      _empty_value = [ ]\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      model_dependencies = [ ]\n",
      "\n",
      "\n",
      "LINE: 2001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      dest as a string 'keep_pot', default as boolean False and help as a string \"Keep .pot file after making messages. Useful when debugging.\"   define the method handle with 3 arguments: self, unpacked list args and unpacked dictionary options.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      dest as a string 'no_obsolete', default as boolean False and help as a string \"Remove obsolete message strings.\".   call the method parser.add_argument with 5 arguments: string '--keep-pot', action as a string 'store_true',\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      parser . add_argument ( '--keep-pot' , action = 'store_true' , dest = 'keep_pot' ,  default = False , help = \"Keep .pot file after making messages. Useful when debugging.\" )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m       def handle ( self , * args , ** options ) :\n",
      "\n",
      "\n",
      "LINE: 2501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      if error is an instance of ValidationError,\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if message is an instance of ValidationError class,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m       if isinstance ( message , ValidationError ) :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     if isinstance ( error , ValidationError ) :\n",
      "\n",
      "\n",
      "LINE: 3001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      call the function escape with an argument text, return the result.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      define the function escape with an argument text.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m        def escape ( text ) :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m              return escape ( text )\n",
      "\n",
      "\n",
      "LINE: 3501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      truncate last 9 elements of filename and append it string \".py\".\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if filename ends with string \"$py.class\",\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m       if filename . endswith ( \"$py.class\" ) :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m                  filename = filename [ : - 9 ] + \".py\"\n",
      "\n",
      "\n",
      "LINE: 4001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      call the function importlib_find with 2 arguments: full_module_name and package_path, if the result is not None, return boolean True,\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if value under the name key of the sys.modules dictionary is not None, return boolean True, otherwise return boolean False.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m                    return sys . modules [ name ] is not None\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     return importlib_find ( full_module_name , package_path ) is not None\n",
      "\n",
      "\n",
      "LINE: 4501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      call the encode function on the Header class instance, created with arguments addr and encoding, substitute the result for addr.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      call the addr.encode method with string 'ascii' as an argument.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               addr . encode ( 'ascii' )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m                  addr = Header ( addr , encoding ) . encode ( )\n",
      "\n",
      "\n",
      "LINE: 5001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      module_values is a set created from the elements of sys.modules.values method return value.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      return value under the name key of the sys.modules dictionary.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      return sys . modules [ name ]\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     module_values = set ( sys . modules . values ( ) )\n",
      "\n",
      "\n",
      "LINE: 5501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      return klass.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      substitute app_name for klass.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m                    klass = app_name\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      return klass\n",
      "\n",
      "\n",
      "LINE: 6001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      raise CommandError exception with string \"This script should be run from the Django Git checkout or your project or app tree, or with the settings module specified.\", as argument.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      raise an TemplateSyntaxError exception with an argument string \"'autoescape' argument should be 'on' or 'off'\".\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               raise TemplateSyntaxError ( \"'autoescape' argument should be 'on' or 'off'\" )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m                  raise CommandError ( \"This script should be run from the Django Git \"  \"checkout or your project or app tree, or with \"  \"the settings module specified.\" )\n",
      "\n",
      "\n",
      "LINE: 6501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      called with string as an argument 'Error executing %s: %s', where '%s' is replaced by: first element of args and strerror, respective,   and third element of the result of the function sys.exc_info.   call the method p.communicate, assign the result to output and errors, respectively.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      call the method sys.stderr.write with an argument string \"ERROR: %s\\n\", where '%s' is replaced by e.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               sys . stderr . write ( \"ERROR: %s\\n\" % e )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      output , errors = p . communicate ( )\n",
      "\n",
      "\n",
      "LINE: 7001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      sum index and length of self._boundary, substitute the result for next.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      substitute next for self.next.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      self . next = next\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     next = index + len ( self . _boundary )\n",
      "\n",
      "\n",
      "LINE: 7501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      objects_in_fixture is integer 0.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      integer 0 and integer 0.   try,\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m      try :\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m     objects_in_fixture = 0\n",
      "\n",
      "\n",
      "LINE: 8001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      raise an ValueError exception with an argument string 'Unknown level: %r' formated with level.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      raise an ValueError with an argument string \"attempted relative import beyond top-level package\".\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m                   raise ValueError ( \"attempted relative import beyond top-level package\" )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m                      raise ValueError ( 'Unknown level: %r' % level )\n",
      "\n",
      "\n",
      "LINE: 8501 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      otherwise if msg and visible_issue_count are true,\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      if lengths of value and arg are equal, return boolean True, otherwise return boolean False.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m               return len ( value ) == int ( arg )\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m      elif msg and visible_issue_count :\n",
      "\n",
      "\n",
      "LINE: 9001 \n",
      "\u001b[38;5;24m\u001b[48;5;85mQUERY:\u001b[0m      substitute Field.creation_counter for self.creation_counter.\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED DESCRIPTION:\u001b[0m      substitute fields for self.fields.\n",
      " \n",
      "\u001b[38;5;24m\u001b[48;5;153mPRED CODE:\u001b[0m       self . fields = fields\n",
      "\n",
      "\u001b[38;5;24m\u001b[48;5;85mTRUTH:\u001b[0m       self . creation_counter = Field . creation_counter\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linecache.clearcache()\n",
    "with open(test_fp + \".desc\", \"r\") as f, open(\"temp/retrieval_predictions\" + \".code\", \"w\") as out:\n",
    "    lines = f.readlines()\n",
    "    l = 1\n",
    "    for line in lines:\n",
    "        result = env.query(p.sub(\" \", line), results_requested=1)\n",
    "        if result != ():\n",
    "            out.write(linecache.getline(train_fp + \".code\",result[0][0]))\n",
    "        else:\n",
    "            out.write(\"\\n\")\n",
    "            print(\"LINE: {} \\n{}{}QUERY:{}    {}\\nSANITIZED QUERY      {}\\n{}{}PRED DESCRIPTION{}:   ######### NO PREDICTION ##########\\n\".format(\n",
    "                l, \n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                line,\n",
    "                fg(24), \n",
    "                bg(217),\n",
    "                attr(0),\n",
    "                line.translate(str.maketrans('', '', string.punctuation))))\n",
    "        if l % 500 == 1:\n",
    "            print(\"LINE: {} \\n{}{}QUERY:{}    {}\\n{}{}PRED DESCRIPTION:{}    {} \\n{}{}PRED CODE:{}     {}\\n{}{}TRUTH:{}    {}\\n\".format(\n",
    "                l, \n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                line, \n",
    "                fg(24), \n",
    "                bg(153),\n",
    "                attr(0),\n",
    "                linecache.getline(train_fp + \".desc\", result[0][0]), \n",
    "                fg(24), \n",
    "                bg(153),\n",
    "                attr(0),\n",
    "                linecache.getline(train_fp + \".code\", result[0][0]),\n",
    "                fg(24), \n",
    "                bg(85),\n",
    "                attr(0),\n",
    "                linecache.getline(test_fp + \".code\", l)))\n",
    "        l += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating BLEU Score\n",
    "We use a librarry from Google's tensor2tensor library. This way we ensure the methods are correct by being actively maintained. The method takes two files and compares them line by line.\n",
    "\n",
    "The only disadvantage is that it doesn't work with TF2.0 yet. Curently using TF1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_uncased =  33.66\n"
     ]
    }
   ],
   "source": [
    "bleu = 100 * bleu_hook.bleu_wrapper(\"temp/retrieval_test.code\", \"temp/retrieval_predictions.code\",\n",
    "                                          case_sensitive=False)\n",
    "print(\"BLEU_uncased = %6.2f\" % bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "In order to make fast optimisations we will use a multiprocessing library to fully utilise our hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    {\n",
    "        \"job_id\":0,\n",
    "        \"train_fp\":\"temp/retrieval_train\",\n",
    "        \"test_fp\":\"temp/retrieval_test\",\n",
    "        \"src_ext\":\".desc\",\n",
    "        \"tgt_ext\":\".code\",\n",
    "        \"hyps\":{\n",
    "            \"b\":0.75,\n",
    "            \"k1\":1.2\n",
    "        },\n",
    "        \"index\":index,\n",
    "        \"folds\":1,\n",
    "        \"fold\":1\n",
    "    },\n",
    "    {\n",
    "        \"job_id\":0,\n",
    "        \"train_fp\":\"temp/retrieval_train\",\n",
    "        \"test_fp\":\"temp/retrieval_test\",\n",
    "        \"src_ext\":\".desc\",\n",
    "        \"tgt_ext\":\".code\",\n",
    "        \"hyps\":{\n",
    "            \"b\":0.75,\n",
    "            \"k1\":1.3\n",
    "        },\n",
    "        \"index\":index,\n",
    "        \"folds\":1,\n",
    "        \"fold\":1\n",
    "    },\n",
    "    {\n",
    "        \"job_id\":0,\n",
    "        \"train_fp\":\"temp/retrieval_train\",\n",
    "        \"test_fp\":\"temp/retrieval_test\",\n",
    "        \"src_ext\":\".desc\",\n",
    "        \"tgt_ext\":\".code\",\n",
    "        \"hyps\":{\n",
    "            \"b\":0.75,\n",
    "            \"k1\":1.4\n",
    "        },\n",
    "        \"index\":index,\n",
    "        \"folds\":1,\n",
    "        \"fold\":1\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(task):\n",
    "    linecache.clearcache()\n",
    "    env = pyndri.TFIDFQueryEnvironment(task[\"index\"], k1=task[\"hyps\"][\"k1\"], b=task[\"hyps\"][\"b\"])\n",
    "    \n",
    "    out_file_name = \"temp/retrieval_predictions_fold:{}-{}_k1:{}_b:{}\".format(\n",
    "                                                                            task[\"fold\"],\n",
    "                                                                            task[\"folds\"],\n",
    "                                                                            task[\"hyps\"][\"k1\"],\n",
    "                                                                            task[\"hyps\"][\"b\"])\n",
    "    with open(task[\"test_fp\"] + \".desc\", \"r\") as f, open(out_file_name + \".code\", \"w\") as out:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            result = env.query(pattern.sub(\" \", line), results_requested=1)\n",
    "            if result != ():\n",
    "                out.write(linecache.getline(task[\"train_fp\"] + \".code\",result[0][0]))\n",
    "            else:\n",
    "                out.write(\"\\n\")\n",
    "                \n",
    "    bleu = 100 * bleu_hook.bleu_wrapper(task[\"test_fp\"] + \".code\", out_file_name + \".code\",\n",
    "                                          case_sensitive=False)\n",
    "                \n",
    "    task[\"BLEU\"] = bleu\n",
    "    measures.append(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Manager() as manager:\n",
    "    measures = manager.list()\n",
    "    processes = []\n",
    "    for task in tasks:\n",
    "        p = Process(target=train_eval, args=(task,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "        measures = list(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'BLEU': 33.36871564388275,\n",
       "  'fold': 1,\n",
       "  'folds': 1,\n",
       "  'hyps': {'b': 0.75, 'k1': 1.3},\n",
       "  'index': <pyndri.Index of 0 documents>,\n",
       "  'job_id': 0,\n",
       "  'src_ext': '.desc',\n",
       "  'test_fp': 'temp/retrieval_test',\n",
       "  'tgt_ext': '.code',\n",
       "  'train_fp': 'temp/retrieval_train'},\n",
       " {'BLEU': 33.23887288570404,\n",
       "  'fold': 1,\n",
       "  'folds': 1,\n",
       "  'hyps': {'b': 0.75, 'k1': 1.4},\n",
       "  'index': <pyndri.Index of 0 documents>,\n",
       "  'job_id': 0,\n",
       "  'src_ext': '.desc',\n",
       "  'test_fp': 'temp/retrieval_test',\n",
       "  'tgt_ext': '.code',\n",
       "  'train_fp': 'temp/retrieval_train'},\n",
       " {'BLEU': 33.663979172706604,\n",
       "  'fold': 1,\n",
       "  'folds': 1,\n",
       "  'hyps': {'b': 0.75, 'k1': 1.2},\n",
       "  'index': <pyndri.Index of 0 documents>,\n",
       "  'job_id': 0,\n",
       "  'src_ext': '.desc',\n",
       "  'test_fp': 'temp/retrieval_test',\n",
       "  'tgt_ext': '.code',\n",
       "  'train_fp': 'temp/retrieval_train'}]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
