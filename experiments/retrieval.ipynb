{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 ____       __       _                 __   ______            _ __      __     __\n",
    "                / __ \\___  / /______(_)__ _   ______ _/ /  / ____/___  ____  (_) /___  / /_   / /\n",
    "               / /_/ / _ \\/ __/ ___/ / _ \\ | / / __ `/ /  / /   / __ \\/ __ \\/ / / __ \\/ __/  / / \n",
    "              / _, _/  __/ /_/ /  / /  __/ |/ / /_/ / /  / /___/ /_/ / /_/ / / / /_/ / /_   /_/  \n",
    "             /_/ |_|\\___/\\__/_/  /_/\\___/|___/\\__,_/_/   \\____/\\____/ .___/_/_/\\____/\\__/  (_)   \n",
    "                                                                   /_/                           \n",
    "                                       (learning every nanosecond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting code\n",
    "The goal of this algorithm is to retrieve a relevant code snippet given an English description and a series of already defined code/description pairs.\n",
    "![ret](../images/retrievalHighLevel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linecache\n",
    "import pyndri\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import re, string, timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indri \n",
    "Indri is the retrieval engine that I am currently using since it has a nice interface with python and has some of the algorithms I need.\n",
    "\n",
    "Indri takes files in an XML format. Sentence pairs are usually stored line by line in a file. So we will need to convert from single line to formatted XML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets \n",
    "We will currently make a full explanation for only one dataset: Django. This is because it is relatively small (18k sentences) and clean. Further descriptions and analysis are found in other notebooks in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(fp, src_ext=\".src\", tgt_ext=\".tgt\", lines=[3,21,80,99]):\n",
    "    linecache.clearcache()\n",
    "    for l in lines:\n",
    "        print(linecache.getline(train_fp+src_ext, l))\n",
    "        print(\"LINE: {} \\nSOURCE:    {} \\nTARGET:     {}\\n\".format(l, \n",
    "                                                                   linecache.getline(fp+src_ext, l), \n",
    "                                                                   linecache.getline(fp+tgt_ext, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 13 \n",
      "SOURCE:      define the function get_cache with backend and dictionary pair of elements kwargs as arguments.\n",
      " \n",
      "TARGET:         def get_cache ( backend , ** kwargs ) :\n",
      "\n",
      "\n",
      "LINE: 14 \n",
      "SOURCE:      call the function warnings.warn with string \"'get_cache' is deprecated in favor of 'caches'.\", RemovedInDjango19Warning,\n",
      " \n",
      "TARGET:      warnings . warn ( \"'get_cache' is deprecated in favor of 'caches'.\" ,  RemovedInDjango19Warning , stacklevel = 2 )\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "django_fp = \"../datasets/django/all\"\n",
    "show_sample(django_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[13,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  from threading import local into default name space.\r\n",
      "  import module warnings.\r\n",
      "  from django.conf import settings into default name space.\r\n",
      "  from django.core import signals into default name space.\r\n",
      "  from django.core.cache.backends.base import InvalidCacheBackendError, CacheKeyWarning and BaseCache into default name space.\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from threading import local\r\n",
      " import warnings\r\n",
      "  from django . conf import settings\r\n",
      " from django . core import signals\r\n",
      " from django . core . cache . backends . base import (  InvalidCacheBackendError , CacheKeyWarning , BaseCache )\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 ../datasets/django/all.code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  temp  Created \n"
     ]
    }
   ],
   "source": [
    "dirName = \"temp\"\n",
    " \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and train / test split\n",
    "Copy the full dataset to the temp folder. We then split the data into a training and testing set at around 90% / 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9 # this means 90% of the data will be used for training, thus 10% for testing\n",
    "num_samples = sum(1 for line in open(django_fp + \".desc\"))\n",
    "train_cutoff = int(num_samples * train_ratio)\n",
    "\n",
    "lines = np.arange(num_samples)\n",
    "np.random.shuffle(lines)\n",
    "\n",
    "train_lines = lines[:train_cutoff]\n",
    "test_lines = lines[train_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "train_fp = \"temp/retrieval_train\"\n",
    "test_fp = \"temp/retrieval_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".code\", \"w\") as out:\n",
    "    for l in train_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test split for .desc and .code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".desc\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".desc\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_fp + \".code\", \"w\") as out:\n",
    "    for l in test_lines:\n",
    "        src = linecache.getline(django_fp + \".code\", l)\n",
    "        out.write(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to TrecText format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_fp + \".desc\", \"r\") as f, open(\"temp/train_desc.trectext\", \"w\") as out:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        \n",
    "        if not line :\n",
    "            break\n",
    "            \n",
    "        out.write(\"<DOC>\\n  <DOCNO>{}</DOCNO>\\n  <TEXT>\\n{}  </TEXT>\\n</DOC>\\n\".format(count, line))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the index with indri\n",
    "To create an index we need to supply Indri with a parameter file specifying how to handle each document. Indri will then generate an index folder with is fast to query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"temp/IndriBuildIndex.conf\", \"w\") as out:\n",
    "    conf = \"\"\"\n",
    "<parameters>\n",
    "<index>temp/django_index/</index>\n",
    "<memory>1024M</memory>\n",
    "<storeDocs>true</storeDocs>\n",
    "<corpus><path>temp/train_desc.trectext</path><class>trectext</class></corpus>\n",
    "<stemmer><name>krovetz</name></stemmer>\n",
    "</parameters>\"\"\"\n",
    "    \n",
    "    out.write(conf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kstem_add_table_entry: Duplicate word emeritus will be ignored.\n",
      "0:00: Created repository temp/django_index/\n",
      "0:00: Opened temp/train_desc.trectext\n",
      "0:06: Documents parsed: 16923 Documents indexed: 16923\n",
      "0:06: Closed temp/train_desc.trectext\n",
      "0:06: Closing index\n",
      "0:07: Finished\n"
     ]
    }
   ],
   "source": [
    "!IndriBuildIndex temp/IndriBuildIndex.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pyndri.Index(\"temp/django_index/\")\n",
    "env = pyndri.TFIDFQueryEnvironment(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  replace every occurrence of '\\\\t' in s with '\\\\\\\\t'.\\n\""
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linecache.getline(train_fp+\".desc\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'temp/retrieval_train.desc'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fp+\".desc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = env.query('error handler', results_requested=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  for every handler in handlers,\n",
      "\n",
      "LINE: 16012 \n",
      "SOURCE:      for every handler in handlers,\n",
      " \n",
      "TARGET:          for handler in handlers :\n",
      "\n",
      "\n",
      "  for every handler in handlers,\n",
      "\n",
      "LINE: 10339 \n",
      "SOURCE:      for every handler in handlers,\n",
      " \n",
      "TARGET:                               for handler in handlers :\n",
      "\n",
      "\n",
      "  substitute self._upload_handlers for handlers.\n",
      "\n",
      "LINE: 8784 \n",
      "SOURCE:      substitute self._upload_handlers for handlers.\n",
      " \n",
      "TARGET:      handlers = self . _upload_handlers\n",
      "\n",
      "\n",
      "  substitute upload_handlers for self._upload_handlers.\n",
      "\n",
      "LINE: 7760 \n",
      "SOURCE:      substitute upload_handlers for self._upload_handlers.\n",
      " \n",
      "TARGET:       self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n",
      "  substitute _upload_handlers for self.__upload_handlers.\n",
      "\n",
      "LINE: 4361 \n",
      "SOURCE:      substitute _upload_handlers for self.__upload_handlers.\n",
      " \n",
      "TARGET:      self . _upload_handlers = upload_handlers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_sample(train_fp, src_ext=\".desc\", tgt_ext=\".code\", lines=[doc[0] for doc in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_code = [linecache.getline(django_fp + \".code\", doc[0]) for doc in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINE: 1 \n",
      "QUERY:       if p_pattern starts with a string '^',\n",
      "\n",
      "PRED DESCRIPTION:      open the file named filepath in read mode, with file descriptor renamed to fp perform,\n",
      " \n",
      "PRED CODE:                       with open ( filepath , 'rb' ) as fp :\n",
      "\n",
      "TRUTH              return commands\n",
      "\n",
      "\n",
      "LINE: 501 \n",
      "QUERY:       return value.\n",
      "\n",
      "PRED DESCRIPTION:      substitute sys.maxsize for MAXSIZE.\n",
      " \n",
      "PRED CODE:       MAXSIZE = sys . maxsize\n",
      "\n",
      "TRUTH     return\n",
      "\n",
      "\n",
      "LINE: 1001 \n",
      "QUERY:       define the method _isdst with 2 arguments self and dt.\n",
      "\n",
      "PRED DESCRIPTION:      define the method eval with 2 arguments self and context.\n",
      " \n",
      "PRED CODE:        def eval ( self , context ) :\n",
      "\n",
      "TRUTH     return changeset\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-b3322ba3fcb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinecache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".code\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             print(\"LINE: {} \\nQUERY:     {}\\nPRED DESCRIPTION:    {} \\nPRED CODE:     {}\\nTRUTH    {}\\n\".format(\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "with open(test_fp + \".desc\", \"r\") as f, open(\"temp/retrieval_predictions\" + \".code\", \"w\") as out:\n",
    "    lines = f.readlines()\n",
    "    l = 1\n",
    "    for line in lines:\n",
    "        result = env.query(line.translate(str.maketrans('', '', string.punctuation)), results_requested=1)\n",
    "        out.write(linecache.getline(train_fp + \".code\",result[0][0]))\n",
    "        if l % 500 == 1:\n",
    "            print(\"LINE: {} \\nQUERY:     {}\\nPRED DESCRIPTION:    {} \\nPRED CODE:     {}\\nTRUTH    {}\\n\".format(\n",
    "                l, \n",
    "                line, \n",
    "                linecache.getline(train_fp + \".desc\", result[0][0]), \n",
    "                linecache.getline(train_fp + \".code\", result[0][0]), \n",
    "                linecache.getline(test_fp + \".code\", l)))\n",
    "        l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if ppattern starts with a string '"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"if p_pattern starts with a string '^',\".translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
